{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj2yGjE4pIYH",
        "colab_type": "code",
        "outputId": "43bcff83-2362-40df-bdcd-3d97fa59e65f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pylab import *\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, MaxPooling2D, Lambda, Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.initializers import glorot_uniform,he_uniform\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTG3dosypayo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_classes = 10\n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "def buildDataSet():\n",
        "    \n",
        "    (x_train_origin, y_train_origin), (x_test_origin, y_test_origin) = mnist.load_data()\n",
        "\n",
        "    assert K.image_data_format() == 'channels_last'\n",
        "    x_train_origin = x_train_origin.reshape(x_train_origin.shape[0], img_rows, img_cols, 1)\n",
        "    x_test_origin = x_test_origin.reshape(x_test_origin.shape[0], img_rows, img_cols, 1)\n",
        "    \n",
        "    dataset_train = []\n",
        "    dataset_test = []\n",
        "    \n",
        "    #Sorting images by classes and normalize values 0=>1\n",
        "    for n in range(nb_classes):\n",
        "        images_class_n = np.asarray([row for idx,row in enumerate(x_train_origin) if y_train_origin[idx]==n])\n",
        "        dataset_train.append(images_class_n/255)\n",
        "        \n",
        "        images_class_n = np.asarray([row for idx,row in enumerate(x_test_origin) if y_test_origin[idx]==n])\n",
        "        dataset_test.append(images_class_n/255)\n",
        "    nw_dataset_train = [dataset_train[i][:4] for i in range(10)]\n",
        "    return nw_dataset_train,dataset_test,x_train_origin,y_train_origin,x_test_origin,y_test_origin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBuPe9zdpevC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "nw_dataset_train,dataset_test,x_train_origin,y_train_origin,x_test_origin,y_test_origin = buildDataSet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QmyxpDi78Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_network(input_shape, embeddingsize):\n",
        "    \n",
        "     # Convolutional Neural Network\n",
        "    network = Sequential()\n",
        "    network.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "    network.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    network.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    network.add(Dropout(0.25))\n",
        "    network.add(Flatten())\n",
        "    network.add(Dense(128, activation='relu'))\n",
        "    network.add(Dropout(0.5))\n",
        "    network.add(Dense(embeddingsize, activation=None,\n",
        "                   kernel_regularizer=l2(1e-3),\n",
        "                   kernel_initializer='he_uniform'))\n",
        "   \n",
        "    #Force the encoding to live on the d-dimentional hypershpere\n",
        "    network.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n",
        "    \n",
        "    return network\n",
        "\n",
        "class TripletLossLayer(Layer):\n",
        "    def __init__(self, alpha, **kwargs):\n",
        "        self.alpha = alpha\n",
        "        super(TripletLossLayer, self).__init__(**kwargs)\n",
        "    \n",
        "    def triplet_loss(self, inputs):\n",
        "        anchor, positive, negative = inputs\n",
        "        p_dist = K.sum(K.square(anchor-positive), axis=-1)\n",
        "        n_dist = K.sum(K.square(anchor-negative), axis=-1)\n",
        "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        loss = self.triplet_loss(inputs)\n",
        "        self.add_loss(loss)\n",
        "        return loss\n",
        "\n",
        "def build_model(input_shape, network, margin=0.2):\n",
        "    \n",
        "     # Define the tensors for the three input images\n",
        "    anchor_input = Input(input_shape, name=\"anchor_input\")\n",
        "    positive_input = Input(input_shape, name=\"positive_input\")\n",
        "    negative_input = Input(input_shape, name=\"negative_input\") \n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the three images\n",
        "    encoded_a = network(anchor_input)\n",
        "    encoded_p = network(positive_input)\n",
        "    encoded_n = network(negative_input)\n",
        "    \n",
        "    #TripletLoss Layer\n",
        "    loss_layer = TripletLossLayer(alpha=margin,name='triplet_loss_layer')([encoded_a,encoded_p,encoded_n])\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    network_train = Model(inputs=[anchor_input,positive_input,negative_input],outputs=loss_layer)\n",
        "    \n",
        "    # return the model\n",
        "    return network_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alRYCtrHBjzP",
        "colab_type": "code",
        "outputId": "28273535-1942-4892-ca93-716b0ce4844d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "network = build_network(input_shape,embeddingsize=10)\n",
        "network_train = build_model(input_shape,network)\n",
        "optimizer = Adam(lr = 0.00007)\n",
        "network_train.compile(loss=None,optimizer=optimizer)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py:819: UserWarning: Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.\n",
            "  'be expecting any data to be passed to {0}.'.format(name))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWNOsTYiHnb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch_random(batch_size):\n",
        "\n",
        "    m, w, h,c = nw_dataset_train[0].shape\n",
        "    \n",
        "    \n",
        "    # initialize result\n",
        "    triplets=[np.zeros((batch_size,h, w,c)) for i in range(3)]\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        #Pick one random class for anchor\n",
        "        anchor_class = np.random.randint(0, nb_classes)\n",
        "        nb_sample_available_for_class_AP = nw_dataset_train[anchor_class].shape[0] # 4\n",
        "        \n",
        "        #Pick two different random pics for this class => A and P\n",
        "        [idx_A,idx_P] = np.random.choice(nb_sample_available_for_class_AP,size=2,replace=False)\n",
        "        \n",
        "        #Pick another class for N, different from anchor_class\n",
        "        negative_class = (anchor_class + np.random.randint(1,nb_classes)) % nb_classes\n",
        "        nb_sample_available_for_class_N = nw_dataset_train[negative_class].shape[0] # 4\n",
        "        \n",
        "        #Pick a random pic for this negative class => N\n",
        "        idx_N = np.random.randint(0, nb_sample_available_for_class_N)\n",
        "\n",
        "        triplets[0][i,:,:,:] = nw_dataset_train[anchor_class][idx_A,:,:,:]\n",
        "        triplets[1][i,:,:,:] = nw_dataset_train[anchor_class][idx_P,:,:,:]\n",
        "        triplets[2][i,:,:,:] = nw_dataset_train[negative_class][idx_N,:,:,:]\n",
        "\n",
        "    return triplets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcmodVnkIhQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_dist(a,b):\n",
        "    return np.sum(np.square(a-b))\n",
        "\n",
        "def get_batch_hard(draw_batch_size,hard_batchs_size,norm_batchs_size,network):\n",
        "    \n",
        "    m, w, h,c = nw_dataset_train[0].shape\n",
        "    \n",
        "    \n",
        "    #Step 1 : pick a random batch to study\n",
        "    studybatch = get_batch_random(draw_batch_size)\n",
        "    \n",
        "    #Step 2 : compute the loss with current network : d(A,P)-d(A,N). The alpha parameter here is omited here since we want only to order them\n",
        "    studybatchloss = np.zeros((draw_batch_size))\n",
        "    \n",
        "    #Compute embeddings for anchors, positive and negatives\n",
        "    A = network.predict(studybatch[0])\n",
        "    P = network.predict(studybatch[1])\n",
        "    N = network.predict(studybatch[2])\n",
        "    \n",
        "    #Compute d(A,P)-d(A,N)\n",
        "    studybatchloss = np.sum(np.square(A-P),axis=1) - np.sum(np.square(A-N),axis=1)\n",
        "    \n",
        "    #Sort by distance (high distance first) and take the \n",
        "    selection = np.argsort(studybatchloss)[::-1][:hard_batchs_size]\n",
        "    \n",
        "    #Draw other random samples from the batch\n",
        "    selection2 = np.random.choice(np.delete(np.arange(draw_batch_size),selection),norm_batchs_size,replace=False)\n",
        "    \n",
        "    selection = np.append(selection,selection2)\n",
        "    \n",
        "    triplets = [studybatch[0][selection,:,:,:], studybatch[1][selection,:,:,:], studybatch[2][selection,:,:,:]]\n",
        "    \n",
        "    return triplets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny7FSYAmIm18",
        "colab_type": "code",
        "outputId": "e1b7b84f-3f3d-4eba-de3d-7185441db1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "print(\"Starting training process!\")\n",
        "print(\"-------------------------------------\")\n",
        "t_start = time.time()\n",
        "for i in range(300):\n",
        "    triplets = get_batch_hard(50,10,10,network)\n",
        "    loss = network_train.train_on_batch(triplets, None)\n",
        "    triplets = get_batch_hard(100,8,8,network)\n",
        "    loss = network_train.train_on_batch(triplets, None)\n",
        "    triplets = get_batch_hard(200,16,16,network)\n",
        "    loss = network_train.train_on_batch(triplets, None)\n",
        "    triplets = get_batch_hard(300,15,15,network)\n",
        "    loss = network_train.train_on_batch(triplets, None)\n",
        "    triplets = get_batch_hard(400,16,16,network)\n",
        "    loss = network_train.train_on_batch(triplets, None)\n",
        "    print(\"Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0,loss,3))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training process!\n",
            "-------------------------------------\n",
            "Time for 0 iterations: 0.0 mins, Train Loss: 11.441378593444824\n",
            "Time for 1 iterations: 0.1 mins, Train Loss: 12.6925048828125\n",
            "Time for 2 iterations: 0.1 mins, Train Loss: 6.295858383178711\n",
            "Time for 3 iterations: 0.1 mins, Train Loss: 5.711435794830322\n",
            "Time for 4 iterations: 0.1 mins, Train Loss: 10.109185218811035\n",
            "Time for 5 iterations: 0.1 mins, Train Loss: 6.8788275718688965\n",
            "Time for 6 iterations: 0.1 mins, Train Loss: 8.028914451599121\n",
            "Time for 7 iterations: 0.1 mins, Train Loss: 7.6599016189575195\n",
            "Time for 8 iterations: 0.1 mins, Train Loss: 5.436262607574463\n",
            "Time for 9 iterations: 0.1 mins, Train Loss: 6.601184844970703\n",
            "Time for 10 iterations: 0.1 mins, Train Loss: 9.051185607910156\n",
            "Time for 11 iterations: 0.1 mins, Train Loss: 5.923454284667969\n",
            "Time for 12 iterations: 0.1 mins, Train Loss: 7.440918922424316\n",
            "Time for 13 iterations: 0.1 mins, Train Loss: 6.48535680770874\n",
            "Time for 14 iterations: 0.1 mins, Train Loss: 5.997874736785889\n",
            "Time for 15 iterations: 0.1 mins, Train Loss: 7.067991733551025\n",
            "Time for 16 iterations: 0.1 mins, Train Loss: 5.3827290534973145\n",
            "Time for 17 iterations: 0.1 mins, Train Loss: 5.635067939758301\n",
            "Time for 18 iterations: 0.1 mins, Train Loss: 5.280795574188232\n",
            "Time for 19 iterations: 0.1 mins, Train Loss: 4.916280746459961\n",
            "Time for 20 iterations: 0.1 mins, Train Loss: 7.1960577964782715\n",
            "Time for 21 iterations: 0.1 mins, Train Loss: 3.7075355052948\n",
            "Time for 22 iterations: 0.1 mins, Train Loss: 3.9616611003875732\n",
            "Time for 23 iterations: 0.1 mins, Train Loss: 4.125657558441162\n",
            "Time for 24 iterations: 0.1 mins, Train Loss: 4.844781398773193\n",
            "Time for 25 iterations: 0.1 mins, Train Loss: 6.181613445281982\n",
            "Time for 26 iterations: 0.1 mins, Train Loss: 4.547958850860596\n",
            "Time for 27 iterations: 0.1 mins, Train Loss: 4.775365352630615\n",
            "Time for 28 iterations: 0.2 mins, Train Loss: 4.100976943969727\n",
            "Time for 29 iterations: 0.2 mins, Train Loss: 4.297327995300293\n",
            "Time for 30 iterations: 0.2 mins, Train Loss: 3.4996337890625\n",
            "Time for 31 iterations: 0.2 mins, Train Loss: 3.388446092605591\n",
            "Time for 32 iterations: 0.2 mins, Train Loss: 4.078000068664551\n",
            "Time for 33 iterations: 0.2 mins, Train Loss: 4.128412246704102\n",
            "Time for 34 iterations: 0.2 mins, Train Loss: 3.352309465408325\n",
            "Time for 35 iterations: 0.2 mins, Train Loss: 3.1429009437561035\n",
            "Time for 36 iterations: 0.2 mins, Train Loss: 2.4119317531585693\n",
            "Time for 37 iterations: 0.2 mins, Train Loss: 1.5757688283920288\n",
            "Time for 38 iterations: 0.2 mins, Train Loss: 1.219980239868164\n",
            "Time for 39 iterations: 0.2 mins, Train Loss: 1.0771524906158447\n",
            "Time for 40 iterations: 0.2 mins, Train Loss: 2.1898036003112793\n",
            "Time for 41 iterations: 0.2 mins, Train Loss: 3.042635679244995\n",
            "Time for 42 iterations: 0.2 mins, Train Loss: 3.1889164447784424\n",
            "Time for 43 iterations: 0.2 mins, Train Loss: 0.9381739497184753\n",
            "Time for 44 iterations: 0.2 mins, Train Loss: 0.9220582246780396\n",
            "Time for 45 iterations: 0.2 mins, Train Loss: 1.7979917526245117\n",
            "Time for 46 iterations: 0.2 mins, Train Loss: 1.1917835474014282\n",
            "Time for 47 iterations: 0.2 mins, Train Loss: 0.7685399651527405\n",
            "Time for 48 iterations: 0.2 mins, Train Loss: 0.40534302592277527\n",
            "Time for 49 iterations: 0.2 mins, Train Loss: 0.023290583863854408\n",
            "Time for 50 iterations: 0.2 mins, Train Loss: 0.1004306972026825\n",
            "Time for 51 iterations: 0.2 mins, Train Loss: 0.21077367663383484\n",
            "Time for 52 iterations: 0.2 mins, Train Loss: 1.542963981628418\n",
            "Time for 53 iterations: 0.2 mins, Train Loss: 0.8158564567565918\n",
            "Time for 54 iterations: 0.3 mins, Train Loss: 0.26317745447158813\n",
            "Time for 55 iterations: 0.3 mins, Train Loss: 0.5938241481781006\n",
            "Time for 56 iterations: 0.3 mins, Train Loss: 0.7621176838874817\n",
            "Time for 57 iterations: 0.3 mins, Train Loss: 0.3820585012435913\n",
            "Time for 58 iterations: 0.3 mins, Train Loss: 0.13934418559074402\n",
            "Time for 59 iterations: 0.3 mins, Train Loss: 0.1917429268360138\n",
            "Time for 60 iterations: 0.3 mins, Train Loss: 0.9371079206466675\n",
            "Time for 61 iterations: 0.3 mins, Train Loss: 0.8063616752624512\n",
            "Time for 62 iterations: 0.3 mins, Train Loss: 1.0504224300384521\n",
            "Time for 63 iterations: 0.3 mins, Train Loss: 0.5149136781692505\n",
            "Time for 64 iterations: 0.3 mins, Train Loss: 0.09750840812921524\n",
            "Time for 65 iterations: 0.3 mins, Train Loss: 0.5502797961235046\n",
            "Time for 66 iterations: 0.3 mins, Train Loss: 0.8934131860733032\n",
            "Time for 67 iterations: 0.3 mins, Train Loss: 0.5441019535064697\n",
            "Time for 68 iterations: 0.3 mins, Train Loss: 0.9705103635787964\n",
            "Time for 69 iterations: 0.3 mins, Train Loss: 1.1138153076171875\n",
            "Time for 70 iterations: 0.3 mins, Train Loss: 0.025835735723376274\n",
            "Time for 71 iterations: 0.3 mins, Train Loss: 0.6017297506332397\n",
            "Time for 72 iterations: 0.3 mins, Train Loss: 0.42595160007476807\n",
            "Time for 73 iterations: 0.3 mins, Train Loss: 0.6423695087432861\n",
            "Time for 74 iterations: 0.3 mins, Train Loss: 1.7172729969024658\n",
            "Time for 75 iterations: 0.3 mins, Train Loss: 0.08173543959856033\n",
            "Time for 76 iterations: 0.3 mins, Train Loss: 1.998227596282959\n",
            "Time for 77 iterations: 0.3 mins, Train Loss: 0.22400686144828796\n",
            "Time for 78 iterations: 0.3 mins, Train Loss: 1.9643677473068237\n",
            "Time for 79 iterations: 0.3 mins, Train Loss: 0.3077954053878784\n",
            "Time for 80 iterations: 0.3 mins, Train Loss: 0.15143941342830658\n",
            "Time for 81 iterations: 0.4 mins, Train Loss: 0.12957437336444855\n",
            "Time for 82 iterations: 0.4 mins, Train Loss: 0.4393022656440735\n",
            "Time for 83 iterations: 0.4 mins, Train Loss: 0.8914547562599182\n",
            "Time for 84 iterations: 0.4 mins, Train Loss: 0.7433724403381348\n",
            "Time for 85 iterations: 0.4 mins, Train Loss: 0.10466508567333221\n",
            "Time for 86 iterations: 0.4 mins, Train Loss: 0.40800461173057556\n",
            "Time for 87 iterations: 0.4 mins, Train Loss: 0.04019978642463684\n",
            "Time for 88 iterations: 0.4 mins, Train Loss: 0.6180514097213745\n",
            "Time for 89 iterations: 0.4 mins, Train Loss: 0.01951858215034008\n",
            "Time for 90 iterations: 0.4 mins, Train Loss: 0.4015260934829712\n",
            "Time for 91 iterations: 0.4 mins, Train Loss: 0.019517619162797928\n",
            "Time for 92 iterations: 0.4 mins, Train Loss: 0.22225287556648254\n",
            "Time for 93 iterations: 0.4 mins, Train Loss: 0.3705868124961853\n",
            "Time for 94 iterations: 0.4 mins, Train Loss: 0.2804390788078308\n",
            "Time for 95 iterations: 0.4 mins, Train Loss: 0.3013496696949005\n",
            "Time for 96 iterations: 0.4 mins, Train Loss: 0.6776155829429626\n",
            "Time for 97 iterations: 0.4 mins, Train Loss: 0.019513927400112152\n",
            "Time for 98 iterations: 0.4 mins, Train Loss: 1.1858711242675781\n",
            "Time for 99 iterations: 0.4 mins, Train Loss: 0.5678036212921143\n",
            "Time for 100 iterations: 0.4 mins, Train Loss: 0.01951235719025135\n",
            "Time for 101 iterations: 0.4 mins, Train Loss: 0.5242225527763367\n",
            "Time for 102 iterations: 0.4 mins, Train Loss: 0.061363860964775085\n",
            "Time for 103 iterations: 0.4 mins, Train Loss: 0.019509602338075638\n",
            "Time for 104 iterations: 0.4 mins, Train Loss: 0.0906258076429367\n",
            "Time for 105 iterations: 0.4 mins, Train Loss: 0.43927016854286194\n",
            "Time for 106 iterations: 0.4 mins, Train Loss: 0.01950669102370739\n",
            "Time for 107 iterations: 0.4 mins, Train Loss: 0.01950584165751934\n",
            "Time for 108 iterations: 0.5 mins, Train Loss: 0.4547610580921173\n",
            "Time for 109 iterations: 0.5 mins, Train Loss: 0.3922116160392761\n",
            "Time for 110 iterations: 0.5 mins, Train Loss: 0.01950322464108467\n",
            "Time for 111 iterations: 0.5 mins, Train Loss: 0.12458568066358566\n",
            "Time for 112 iterations: 0.5 mins, Train Loss: 0.019501589238643646\n",
            "Time for 113 iterations: 0.5 mins, Train Loss: 0.019500816240906715\n",
            "Time for 114 iterations: 0.5 mins, Train Loss: 0.1107146143913269\n",
            "Time for 115 iterations: 0.5 mins, Train Loss: 0.019499193876981735\n",
            "Time for 116 iterations: 0.5 mins, Train Loss: 0.24133728444576263\n",
            "Time for 117 iterations: 0.5 mins, Train Loss: 0.34567734599113464\n",
            "Time for 118 iterations: 0.5 mins, Train Loss: 0.019496941938996315\n",
            "Time for 119 iterations: 0.5 mins, Train Loss: 0.2726322412490845\n",
            "Time for 120 iterations: 0.5 mins, Train Loss: 0.01949564926326275\n",
            "Time for 121 iterations: 0.5 mins, Train Loss: 0.22042644023895264\n",
            "Time for 122 iterations: 0.5 mins, Train Loss: 0.019494304433465004\n",
            "Time for 123 iterations: 0.5 mins, Train Loss: 0.2293521910905838\n",
            "Time for 124 iterations: 0.5 mins, Train Loss: 0.053335197269916534\n",
            "Time for 125 iterations: 0.5 mins, Train Loss: 0.08050490915775299\n",
            "Time for 126 iterations: 0.5 mins, Train Loss: 0.845538318157196\n",
            "Time for 127 iterations: 0.5 mins, Train Loss: 0.34761831164360046\n",
            "Time for 128 iterations: 0.5 mins, Train Loss: 0.577111542224884\n",
            "Time for 129 iterations: 0.5 mins, Train Loss: 0.0700002908706665\n",
            "Time for 130 iterations: 0.5 mins, Train Loss: 0.019490445032715797\n",
            "Time for 131 iterations: 0.5 mins, Train Loss: 0.023665254935622215\n",
            "Time for 132 iterations: 0.5 mins, Train Loss: 0.019489439204335213\n",
            "Time for 133 iterations: 0.5 mins, Train Loss: 0.1714077591896057\n",
            "Time for 134 iterations: 0.6 mins, Train Loss: 0.019488096237182617\n",
            "Time for 135 iterations: 0.6 mins, Train Loss: 0.085609070956707\n",
            "Time for 136 iterations: 0.6 mins, Train Loss: 0.1484619677066803\n",
            "Time for 137 iterations: 0.6 mins, Train Loss: 0.03549497574567795\n",
            "Time for 138 iterations: 0.6 mins, Train Loss: 0.15060731768608093\n",
            "Time for 139 iterations: 0.6 mins, Train Loss: 0.6465563178062439\n",
            "Time for 140 iterations: 0.6 mins, Train Loss: 0.01948441192507744\n",
            "Time for 141 iterations: 0.6 mins, Train Loss: 0.14372949302196503\n",
            "Time for 142 iterations: 0.6 mins, Train Loss: 0.47618621587753296\n",
            "Time for 143 iterations: 0.6 mins, Train Loss: 0.019482837989926338\n",
            "Time for 144 iterations: 0.6 mins, Train Loss: 0.11580324918031693\n",
            "Time for 145 iterations: 0.6 mins, Train Loss: 0.019481593742966652\n",
            "Time for 146 iterations: 0.6 mins, Train Loss: 0.01948085054755211\n",
            "Time for 147 iterations: 0.6 mins, Train Loss: 0.019480155780911446\n",
            "Time for 148 iterations: 0.6 mins, Train Loss: 0.01947944611310959\n",
            "Time for 149 iterations: 0.6 mins, Train Loss: 0.01947869174182415\n",
            "Time for 150 iterations: 0.6 mins, Train Loss: 0.20627613365650177\n",
            "Time for 151 iterations: 0.6 mins, Train Loss: 0.060148730874061584\n",
            "Time for 152 iterations: 0.6 mins, Train Loss: 0.019476160407066345\n",
            "Time for 153 iterations: 0.6 mins, Train Loss: 0.019475340843200684\n",
            "Time for 154 iterations: 0.6 mins, Train Loss: 0.064431332051754\n",
            "Time for 155 iterations: 0.6 mins, Train Loss: 0.04162881523370743\n",
            "Time for 156 iterations: 0.6 mins, Train Loss: 0.14799156785011292\n",
            "Time for 157 iterations: 0.6 mins, Train Loss: 0.019471878185868263\n",
            "Time for 158 iterations: 0.6 mins, Train Loss: 0.01947116106748581\n",
            "Time for 159 iterations: 0.6 mins, Train Loss: 0.26733100414276123\n",
            "Time for 160 iterations: 0.6 mins, Train Loss: 0.27475497126579285\n",
            "Time for 161 iterations: 0.7 mins, Train Loss: 0.13511325418949127\n",
            "Time for 162 iterations: 0.7 mins, Train Loss: 0.019467715173959732\n",
            "Time for 163 iterations: 0.7 mins, Train Loss: 0.06527098268270493\n",
            "Time for 164 iterations: 0.7 mins, Train Loss: 0.22033894062042236\n",
            "Time for 165 iterations: 0.7 mins, Train Loss: 0.019465459510684013\n",
            "Time for 166 iterations: 0.7 mins, Train Loss: 0.01946481503546238\n",
            "Time for 167 iterations: 0.7 mins, Train Loss: 0.6661691069602966\n",
            "Time for 168 iterations: 0.7 mins, Train Loss: 0.019463367760181427\n",
            "Time for 169 iterations: 0.7 mins, Train Loss: 0.5786275267601013\n",
            "Time for 170 iterations: 0.7 mins, Train Loss: 0.019461965188384056\n",
            "Time for 171 iterations: 0.7 mins, Train Loss: 0.019461199641227722\n",
            "Time for 172 iterations: 0.7 mins, Train Loss: 0.01946038194000721\n",
            "Time for 173 iterations: 0.7 mins, Train Loss: 0.01945948600769043\n",
            "Time for 174 iterations: 0.7 mins, Train Loss: 0.3058064579963684\n",
            "Time for 175 iterations: 0.7 mins, Train Loss: 0.38484248518943787\n",
            "Time for 176 iterations: 0.7 mins, Train Loss: 0.01945708505809307\n",
            "Time for 177 iterations: 0.7 mins, Train Loss: 0.0194562878459692\n",
            "Time for 178 iterations: 0.7 mins, Train Loss: 0.16130758821964264\n",
            "Time for 179 iterations: 0.7 mins, Train Loss: 0.2388487309217453\n",
            "Time for 180 iterations: 0.7 mins, Train Loss: 0.019454075023531914\n",
            "Time for 181 iterations: 0.7 mins, Train Loss: 0.01945341005921364\n",
            "Time for 182 iterations: 0.7 mins, Train Loss: 0.019452769309282303\n",
            "Time for 183 iterations: 0.7 mins, Train Loss: 0.019452068954706192\n",
            "Time for 184 iterations: 0.7 mins, Train Loss: 0.019451415166258812\n",
            "Time for 185 iterations: 0.7 mins, Train Loss: 0.01945081166923046\n",
            "Time for 186 iterations: 0.7 mins, Train Loss: 0.0194502342492342\n",
            "Time for 187 iterations: 0.8 mins, Train Loss: 0.019449645653367043\n",
            "Time for 188 iterations: 0.8 mins, Train Loss: 0.019449016079306602\n",
            "Time for 189 iterations: 0.8 mins, Train Loss: 0.05681519955396652\n",
            "Time for 190 iterations: 0.8 mins, Train Loss: 0.019447481259703636\n",
            "Time for 191 iterations: 0.8 mins, Train Loss: 0.019446706399321556\n",
            "Time for 192 iterations: 0.8 mins, Train Loss: 0.16591273248195648\n",
            "Time for 193 iterations: 0.8 mins, Train Loss: 0.019445089623332024\n",
            "Time for 194 iterations: 0.8 mins, Train Loss: 0.019444234669208527\n",
            "Time for 195 iterations: 0.8 mins, Train Loss: 0.019443277269601822\n",
            "Time for 196 iterations: 0.8 mins, Train Loss: 0.01944241113960743\n",
            "Time for 197 iterations: 0.8 mins, Train Loss: 0.019441640004515648\n",
            "Time for 198 iterations: 0.8 mins, Train Loss: 0.019440948963165283\n",
            "Time for 199 iterations: 0.8 mins, Train Loss: 0.02810560166835785\n",
            "Time for 200 iterations: 0.8 mins, Train Loss: 0.30270493030548096\n",
            "Time for 201 iterations: 0.8 mins, Train Loss: 0.019438719376921654\n",
            "Time for 202 iterations: 0.8 mins, Train Loss: 0.019437942653894424\n",
            "Time for 203 iterations: 0.8 mins, Train Loss: 0.01943724788725376\n",
            "Time for 204 iterations: 0.8 mins, Train Loss: 0.01943649724125862\n",
            "Time for 205 iterations: 0.8 mins, Train Loss: 0.019435707479715347\n",
            "Time for 206 iterations: 0.8 mins, Train Loss: 0.019434910267591476\n",
            "Time for 207 iterations: 0.8 mins, Train Loss: 0.03234841674566269\n",
            "Time for 208 iterations: 0.8 mins, Train Loss: 0.12237533926963806\n",
            "Time for 209 iterations: 0.8 mins, Train Loss: 0.2701641321182251\n",
            "Time for 210 iterations: 0.8 mins, Train Loss: 0.019431840628385544\n",
            "Time for 211 iterations: 0.8 mins, Train Loss: 0.023031022399663925\n",
            "Time for 212 iterations: 0.8 mins, Train Loss: 0.019430233165621758\n",
            "Time for 213 iterations: 0.8 mins, Train Loss: 0.019429443404078484\n",
            "Time for 214 iterations: 0.9 mins, Train Loss: 0.019428661093115807\n",
            "Time for 215 iterations: 0.9 mins, Train Loss: 0.06321758031845093\n",
            "Time for 216 iterations: 0.9 mins, Train Loss: 0.019427036866545677\n",
            "Time for 217 iterations: 0.9 mins, Train Loss: 0.019426191225647926\n",
            "Time for 218 iterations: 0.9 mins, Train Loss: 0.01942536048591137\n",
            "Time for 219 iterations: 0.9 mins, Train Loss: 0.6978825926780701\n",
            "Time for 220 iterations: 0.9 mins, Train Loss: 0.05332767218351364\n",
            "Time for 221 iterations: 0.9 mins, Train Loss: 0.30062150955200195\n",
            "Time for 222 iterations: 0.9 mins, Train Loss: 0.20043687522411346\n",
            "Time for 223 iterations: 0.9 mins, Train Loss: 0.04021555185317993\n",
            "Time for 224 iterations: 0.9 mins, Train Loss: 0.01942005567252636\n",
            "Time for 225 iterations: 0.9 mins, Train Loss: 0.01941911317408085\n",
            "Time for 226 iterations: 0.9 mins, Train Loss: 0.019418302923440933\n",
            "Time for 227 iterations: 0.9 mins, Train Loss: 0.2758283019065857\n",
            "Time for 228 iterations: 0.9 mins, Train Loss: 0.019416568800807\n",
            "Time for 229 iterations: 0.9 mins, Train Loss: 0.2964850664138794\n",
            "Time for 230 iterations: 0.9 mins, Train Loss: 0.019414817914366722\n",
            "Time for 231 iterations: 0.9 mins, Train Loss: 0.1788026988506317\n",
            "Time for 232 iterations: 0.9 mins, Train Loss: 0.01941332034766674\n",
            "Time for 233 iterations: 0.9 mins, Train Loss: 0.050288788974285126\n",
            "Time for 234 iterations: 0.9 mins, Train Loss: 0.15710999071598053\n",
            "Time for 235 iterations: 0.9 mins, Train Loss: 0.3987981975078583\n",
            "Time for 236 iterations: 0.9 mins, Train Loss: 0.019410617649555206\n",
            "Time for 237 iterations: 0.9 mins, Train Loss: 0.01941007748246193\n",
            "Time for 238 iterations: 0.9 mins, Train Loss: 0.2649964988231659\n",
            "Time for 239 iterations: 0.9 mins, Train Loss: 0.34609150886535645\n",
            "Time for 240 iterations: 1.0 mins, Train Loss: 0.5238029956817627\n",
            "Time for 241 iterations: 1.0 mins, Train Loss: 0.3144083619117737\n",
            "Time for 242 iterations: 1.0 mins, Train Loss: 0.1967078298330307\n",
            "Time for 243 iterations: 1.0 mins, Train Loss: 0.1012098491191864\n",
            "Time for 244 iterations: 1.0 mins, Train Loss: 0.20084413886070251\n",
            "Time for 245 iterations: 1.0 mins, Train Loss: 0.019403913989663124\n",
            "Time for 246 iterations: 1.0 mins, Train Loss: 0.025697145611047745\n",
            "Time for 247 iterations: 1.0 mins, Train Loss: 0.019402382895350456\n",
            "Time for 248 iterations: 1.0 mins, Train Loss: 0.019401604309678078\n",
            "Time for 249 iterations: 1.0 mins, Train Loss: 0.019400738179683685\n",
            "Time for 250 iterations: 1.0 mins, Train Loss: 0.2147873342037201\n",
            "Time for 251 iterations: 1.0 mins, Train Loss: 0.30569255352020264\n",
            "Time for 252 iterations: 1.0 mins, Train Loss: 0.019398467615246773\n",
            "Time for 253 iterations: 1.0 mins, Train Loss: 0.13327139616012573\n",
            "Time for 254 iterations: 1.0 mins, Train Loss: 0.019396867603063583\n",
            "Time for 255 iterations: 1.0 mins, Train Loss: 0.019396133720874786\n",
            "Time for 256 iterations: 1.0 mins, Train Loss: 0.01939542032778263\n",
            "Time for 257 iterations: 1.0 mins, Train Loss: 0.01939474605023861\n",
            "Time for 258 iterations: 1.0 mins, Train Loss: 0.596157968044281\n",
            "Time for 259 iterations: 1.0 mins, Train Loss: 0.019393298774957657\n",
            "Time for 260 iterations: 1.0 mins, Train Loss: 0.019392414018511772\n",
            "Time for 261 iterations: 1.0 mins, Train Loss: 0.020930243656039238\n",
            "Time for 262 iterations: 1.0 mins, Train Loss: 0.21781094372272491\n",
            "Time for 263 iterations: 1.0 mins, Train Loss: 0.01938941329717636\n",
            "Time for 264 iterations: 1.0 mins, Train Loss: 0.01938847079873085\n",
            "Time for 265 iterations: 1.0 mins, Train Loss: 0.09791413694620132\n",
            "Time for 266 iterations: 1.1 mins, Train Loss: 0.01938674971461296\n",
            "Time for 267 iterations: 1.1 mins, Train Loss: 0.01938607171177864\n",
            "Time for 268 iterations: 1.1 mins, Train Loss: 0.019385404884815216\n",
            "Time for 269 iterations: 1.1 mins, Train Loss: 0.019384710118174553\n",
            "Time for 270 iterations: 1.1 mins, Train Loss: 0.019383979961276054\n",
            "Time for 271 iterations: 1.1 mins, Train Loss: 0.35018354654312134\n",
            "Time for 272 iterations: 1.1 mins, Train Loss: 0.0627928078174591\n",
            "Time for 273 iterations: 1.1 mins, Train Loss: 0.01938185840845108\n",
            "Time for 274 iterations: 1.1 mins, Train Loss: 0.19720543920993805\n",
            "Time for 275 iterations: 1.1 mins, Train Loss: 0.0193804781883955\n",
            "Time for 276 iterations: 1.1 mins, Train Loss: 0.01937972754240036\n",
            "Time for 277 iterations: 1.1 mins, Train Loss: 0.01937895081937313\n",
            "Time for 278 iterations: 1.1 mins, Train Loss: 0.019378185272216797\n",
            "Time for 279 iterations: 1.1 mins, Train Loss: 0.019377298653125763\n",
            "Time for 280 iterations: 1.1 mins, Train Loss: 0.01937638223171234\n",
            "Time for 281 iterations: 1.1 mins, Train Loss: 0.07202085852622986\n",
            "Time for 282 iterations: 1.1 mins, Train Loss: 0.01937483437359333\n",
            "Time for 283 iterations: 1.1 mins, Train Loss: 0.019374096766114235\n",
            "Time for 284 iterations: 1.1 mins, Train Loss: 0.2945786714553833\n",
            "Time for 285 iterations: 1.1 mins, Train Loss: 0.03002283349633217\n",
            "Time for 286 iterations: 1.1 mins, Train Loss: 0.01937202177941799\n",
            "Time for 287 iterations: 1.1 mins, Train Loss: 0.01937120407819748\n",
            "Time for 288 iterations: 1.1 mins, Train Loss: 0.019370263442397118\n",
            "Time for 289 iterations: 1.1 mins, Train Loss: 0.04778257757425308\n",
            "Time for 290 iterations: 1.1 mins, Train Loss: 0.019368493929505348\n",
            "Time for 291 iterations: 1.1 mins, Train Loss: 0.01936766318976879\n",
            "Time for 292 iterations: 1.2 mins, Train Loss: 0.01936679147183895\n",
            "Time for 293 iterations: 1.2 mins, Train Loss: 0.019365893676877022\n",
            "Time for 294 iterations: 1.2 mins, Train Loss: 0.019364941865205765\n",
            "Time for 295 iterations: 1.2 mins, Train Loss: 0.01936405338346958\n",
            "Time for 296 iterations: 1.2 mins, Train Loss: 0.019363194704055786\n",
            "Time for 297 iterations: 1.2 mins, Train Loss: 0.019362257793545723\n",
            "Time for 298 iterations: 1.2 mins, Train Loss: 0.019361432641744614\n",
            "Time for 299 iterations: 1.2 mins, Train Loss: 0.019360624253749847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JCbnngGoYr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_minimal_distances(network, images, refidx):\n",
        "    \n",
        "    predicted_values = []\n",
        "    N=4\n",
        "    _, w,h,c = dataset_test[0].shape\n",
        "    nbimages=images.shape[0]\n",
        "    \n",
        "    #generates embedings for given images\n",
        "    image_embedings = network.predict(images)\n",
        "    \n",
        "    #generates embedings for reference images\n",
        "    ref_images = np.zeros((nb_classes,w,h,c))\n",
        "    for i in range(nb_classes):\n",
        "        ref_images[i,:,:,:] = nw_dataset_train[i][refidx,:,:,:]\n",
        "    ref_embedings = network.predict(ref_images)\n",
        "            \n",
        "    for i in range(nbimages):\n",
        "        \n",
        "        distances = []\n",
        "        for ref in range(nb_classes):\n",
        "            #Compute distance between this images and references\n",
        "            dist = compute_dist(image_embedings[i,:],ref_embedings[ref,:])\n",
        "            distances.append(dist)\n",
        "        predicted_values.append(argmin(distances))\n",
        "    return predicted_values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7TRKjxGLZqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(network):\n",
        "  counts = []\n",
        "\n",
        "  for j in range(100):\n",
        "    predictions = []\n",
        "    reals = []\n",
        "    for index in np.random.randint(0,10000,100):\n",
        "      reals.append(y_test_origin[index])\n",
        "      preds = get_minimal_distances(network,np.expand_dims(x_test_origin[index,:,:,:],axis=0), np.random.randint(0,4))\n",
        "      predictions.append(preds)\n",
        "\n",
        "    count = 0\n",
        "    for k in range(100):\n",
        "      if predictions[k] == reals[k]:\n",
        "        count=count +1\n",
        "    counts.append(count)\n",
        "  return mean(counts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afMh7QKnLsIP",
        "colab_type": "code",
        "outputId": "c75ec2cf-4633-4c58-c4c4-935e477ff044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"algorithm accuracy: \")\n",
        "print(compute_accuracy(network))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "algorithm accuracy: \n",
            "67.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBFFtfLWw7I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}